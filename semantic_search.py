"""
è½»é‡çº§è¯­ä¹‰æœç´¢ç®¡ç†å™¨
ä½¿ç”¨ TF-IDF + ä½™å¼¦ç›¸ä¼¼åº¦å®ç°è¯­ä¹‰æœç´¢
æ— éœ€æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œé€Ÿåº¦å¿«ï¼Œèµ„æºå ç”¨å°ï¼Œä»…ä¾èµ– jieba åˆ†è¯
"""

import jieba
import math
from collections import Counter
from typing import List, Tuple, Dict
from backend.logger import logger


class SemanticSearchManager:
    def __init__(self):
        """åˆå§‹åŒ–åˆ†è¯å™¨"""
        logger.info("âœ… åˆå§‹åŒ–è½»é‡çº§è¯­ä¹‰æœç´¢")
        jieba.setLogLevel(jieba.logging.INFO)
        # æ·»åŠ è‡ªå®šä¹‰è¯å…¸ï¼ˆå¸¸è§åœç”¨è¯ï¼‰
        self.stopwords = set([
            'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±',
            'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'å—',
            'å•Š', 'å‘¢', 'å§', 'ä¹ˆ', 'ä»€ä¹ˆ', 'è¿™', 'é‚£', 'è¿™ä¸ª'
        ])

    def add_memory(self, memory_id: int, content: str, tag: str):
        """æ·»åŠ è®°å¿†åˆ°ç´¢å¼• (è½»é‡çº§ç‰ˆæœ¬æ— éœ€æŒä¹…åŒ–ç´¢å¼•ï¼Œæ­¤æ–¹æ³•ä¸ºç©º)"""
        pass

    def tokenize(self, text: str) -> List[str]:
        """åˆ†è¯å¹¶è¿‡æ»¤åœç”¨è¯"""
        words = jieba.lcut(text.lower())
        return [w for w in words if w not in self.stopwords and len(w) > 1]

    def compute_tf(self, words: List[str]) -> Dict[str, float]:
        """è®¡ç®—è¯é¢‘TF"""
        word_count = Counter(words)
        total = len(words) if words else 1
        return {word: count / total for word, count in word_count.items()}

    def compute_idf(self, documents: List[List[str]]) -> Dict[str, float]:
        """è®¡ç®—é€†æ–‡æ¡£é¢‘ç‡IDF"""
        doc_count = len(documents)
        if doc_count == 0:
            return {}

        word_doc_count = Counter()
        for doc in documents:
            unique_words = set(doc)
            for word in unique_words:
                word_doc_count[word] += 1

        idf = {}
        for word, count in word_doc_count.items():
            idf[word] = math.log(doc_count / (count + 1))

        return idf

    def compute_tfidf(
        self,
        text: str,
        idf: Dict[str, float]
    ) -> Dict[str, float]:
        """è®¡ç®—TF-IDFå‘é‡"""
        words = self.tokenize(text)
        tf = self.compute_tf(words)

        tfidf = {}
        for word, tf_value in tf.items():
            idf_value = idf.get(word, 0)
            tfidf[word] = tf_value * idf_value

        return tfidf

    def cosine_similarity(
        self,
        vec1: Dict[str, float],
        vec2: Dict[str, float]
    ) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        # è·å–æ‰€æœ‰è¯
        all_words = set(vec1.keys()) | set(vec2.keys())

        if not all_words:
            return 0.0

        # è®¡ç®—ç‚¹ç§¯
        dot_product = sum(vec1.get(w, 0) * vec2.get(w, 0) for w in all_words)

        # è®¡ç®—æ¨¡
        norm1 = math.sqrt(sum(v ** 2 for v in vec1.values()))
        norm2 = math.sqrt(sum(v ** 2 for v in vec2.values()))

        if norm1 == 0 or norm2 == 0:
            return 0.0

        return dot_product / (norm1 * norm2)

    def search(
        self,
        query: str,
        documents: List[Tuple[int, str]],
        top_k: int = 5,
        min_score: float = 0.1
    ) -> List[Tuple[int, float]]:
        """
        æœç´¢æœ€ç›¸å…³çš„æ–‡æ¡£

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: [(id, text), ...] æ–‡æ¡£åˆ—è¡¨
            top_k: è¿”å›å‰Kä¸ª
            min_score: æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            [(id, score), ...] æŒ‰ç›¸ä¼¼åº¦é™åº
        """
        if not documents:
            return []

        # åˆ†è¯æ‰€æœ‰æ–‡æ¡£
        doc_words = [self.tokenize(text) for _, text in documents]
        query_words = self.tokenize(query)

        if not query_words:
            return []

        # è®¡ç®—IDF
        all_docs = doc_words + [query_words]
        idf = self.compute_idf(all_docs)

        # è®¡ç®—æŸ¥è¯¢çš„TF-IDF
        query_tfidf = self.compute_tfidf(query, idf)

        # è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„ç›¸ä¼¼åº¦
        results = []
        for (doc_id, doc_text), doc_word_list in zip(documents, doc_words):
            doc_tf = self.compute_tf(doc_word_list)
            # è®¡ç®—æ–‡æ¡£çš„TF-IDF
            doc_tfidf = {
                w: doc_tf.get(w, 0) * idf.get(w, 0)
                for w in set(doc_word_list)
            }

            score = self.cosine_similarity(query_tfidf, doc_tfidf)

            if score >= min_score:
                results.append((doc_id, score))

        # æŒ‰åˆ†æ•°é™åºæ’åº
        results.sort(key=lambda x: x[1], reverse=True)

        return results[:top_k]


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•è¯­ä¹‰æœç´¢ç®¡ç†å™¨\n")

    sm = SemanticSearchManager()

    # æµ‹è¯•æ–‡æ¡£
    documents = [
        (1, "ç”¨æˆ·å§“åï¼šé«˜é¹"),
        (2, "ç”¨æˆ·å¹´é¾„ï¼š41å²"),
        (3, "ç”¨æˆ·ç”Ÿæ—¥ï¼š2æœˆ8æ—¥"),
        (4, "ç”¨æˆ·å–œæ¬¢å†°ç¾å¼"),
        (5, "ç”¨æˆ·ä¸å–œæ¬¢ä½“è‚²è¿åŠ¨"),
        (6, "ç”¨æˆ·å–œæ¬¢å–å’–å•¡")
    ]

    # æµ‹è¯•æŸ¥è¯¢
    queries = [
        "é«˜é¹",
        "å¤šå¤§",
        "41å²",
        "ç”Ÿæ—¥",
        "è¿åŠ¨çˆ±å¥½",
        "å’–å•¡"
    ]

    print("=" * 50)
    for query in queries:
        print(f"æŸ¥è¯¢: '{query}'")
        results = sm.search(query, documents, top_k=3, min_score=0.05)

        if results:
            for doc_id, score in results:
                doc_text = next(text for id, text in documents if id == doc_id)
                print(f"  [{score:.3f}] {doc_text}")
        else:
            print("  æ— åŒ¹é…ç»“æœ")
        print()
